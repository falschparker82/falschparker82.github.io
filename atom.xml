<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Hamann Distributed]]></title>
  <link href="http://distributed.hamann.se/atom.xml" rel="self"/>
  <link href="http://distributed.hamann.se/"/>
  <updated>2013-06-01T22:35:57+02:00</updated>
  <id>http://distributed.hamann.se/</id>
  <author>
    <name><![CDATA[Dominik Hamann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Cache invalidation made easy]]></title>
    <link href="http://distributed.hamann.se/blog/2013/05/31/cache-invalidation-made-easy/"/>
    <updated>2013-05-31T08:13:00+02:00</updated>
    <id>http://distributed.hamann.se/blog/2013/05/31/cache-invalidation-made-easy</id>
    <content type="html"><![CDATA[<blockquote><p>&#8220;There are only two hard problems in Computer Science: cache invalidation and naming things.&#8221;</p><footer><strong>Phil Karlton</strong></footer></blockquote>


<p>Full disclaimer: No, I didn&rsquo;t find the perfect solution either (guess it&rsquo;s an NP hard problem). For a lot of use cases, one of the generically applicable patterns I like best is explained very well by <a href="http://37signals.com/svn/posts/3113-how-key-based-cache-expiration-works">DHH</a> &ndash; my problem with it was that it does not cover the case when entity content changes under the very same ID. I will show you another nice and generic pattern for just this purpose for you to have another trick up your sleeve.</p>

<p>For us, the key problem was about distributed tracking applications caching metadata about the campaigns or images we&rsquo;re delivering. For example, if a campaign manager needs to change the URL, the tracking application needs to redirect to another target. As the tracking is high volume, caching was a no-brainer. For cache invalidation, we settled for a pull approach of one minute refreshes from the database first, which could unfortunately serve stale data and obviously wouldn&rsquo;t scale with growing numbers of entries and servers.</p>

<p>Now instead of custom-building something ourselves, we thought of a more generic approach &ndash; and for us, it boiled down to a dead simple convention.</p>

<h4>How it works</h4>

<p>First step, set up a messaging service. If you&rsquo;re on Amazon like us, <a href="http://aws.amazon.com/de/sns/">SNS</a> (+<a href="http://aws.amazon.com/de/sqs/">SQS</a> maybe) fits the bill perfectly and is set up in minutes, otherwise you might consider <a href="http://www.rabbitmq.com/">RabbitMQ</a> or any AMQP provider.</p>

<p>Second step, create one topic for every entity with the same name.</p>

<p>Third step, follow an easy convention:</p>

<ul>
<li>Everyone who is mutating this entity (with us, it&rsquo;s just the API, making things even easier) publishes the entity ID to the topic after writing to the database.</li>
<li>Everyone who is caching this entity subscribes to the topic and re-pulls the instance when a &ldquo;dirty&rdquo; ID comes in.</li>
</ul>


<h5>This approach is so beautiful for a lot of reasons:</h5>

<ul>
<li>Loose coupling. There are no hard dependencies between any apps following this pattern, but they all work together magically with no stale caches</li>
<li>Very easy to convert from a pull-based system</li>
<li>No to low security hassle. Data never goes through the queue, only IDs</li>
<li>Added plus for SNS users: You can safely leave the dirty work of monitoring and reliably keeping up a messaging system to Amazon.</li>
</ul>


<p>Caveat lector: Obviously this approach will probably only be viable for medium- to low-volume core and master data. For caching and invalidation at several orders of magnitude higher, you&rsquo;d probably be looking at specialized solutions and optimized modelling (Cassandra, S4 and the likes). Also, be careful as this eventually consistent solution still has some low but unknown amount of time where the cache will be stale. So if you&rsquo;re <em>absolutely dependent</em> on consistency for a problem domain, there are few other ways than disabling caching altogether.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Careful with Cassandra upserts]]></title>
    <link href="http://distributed.hamann.se/blog/2013/05/22/careful-with-cassandra-upserts/"/>
    <updated>2013-05-22T08:13:00+02:00</updated>
    <id>http://distributed.hamann.se/blog/2013/05/22/careful-with-cassandra-upserts</id>
    <content type="html"><![CDATA[<p>A nice thing about Cassandra is the easily understandable data model: There are just upserts &ndash; an insert will automatically update / overwrite old rows. This does NOT hold true however in every case when using <em>dynamic</em> columns, as Cassandra does not have the same concept of a &ldquo;row&rdquo; as a traditional database.</p>

<p>Essentially, a Cassandra &ldquo;row&rdquo; is just a double hashmap. One layer goes to the key and says exactly <em>on which server</em> the row is, and the column key says <em>where on the server</em> the column is. This very flexible concept can lead to a problem later on though when some of the columns are different.</p>

<p>Here&rsquo;s an entry in the &ldquo;Employees&rdquo; ColumnFamily:</p>

<pre><code>employee_id: 599 (KEY)
name: "Larry Page"
age: 46
</code></pre>

<p>Now for various reasons, we have to update employee 599 with another denormalized person:</p>

<pre><code>employee_id: 599 (KEY)
name: "Sylvie Stone"
devices: ["MacBook Pro"]
</code></pre>

<p>Sylvie didn&rsquo;t tell us her age (she&rsquo;s a lady after all!) and for new employees, we&rsquo;re also tracking the devices we handed them. When we&rsquo;re upserting employee 599, a lot of people with SQL or a document-oriented database background are expecting to have the second entry in the database. That&rsquo;s not true at all unfortunately &ndash; what we will find now is this:</p>

<pre><code>employee_id: 599 (KEY)
name: "Sylvie Stone"
devices: ["MacBook Pro"]
age: 46
</code></pre>

<p>Welcome to the world of column-oriented databases &ndash; and before you think &ldquo;WTF&rdquo;, think about it for a moment. This is expected behaviour and part of Cassandra&rsquo;s &ldquo;independent columns&rdquo; paradigm. Even if it looks like it in CQL, you never actually overwrite rows &ndash; you overwrite the columns behind it.</p>

<p>So how to avoid this? You just need to model your data properly or navigate around it. As Cassandra columns are way smarter than columns in other databases, there exists a way to correct for this effect in case it&rsquo;s needed. How? Look under the hood. What Cassandra really stores is this:</p>

<pre><code>"599": [
    {name:employee_id, value:599, timestamp: 1340385863990010, ttl: 0},
    {name:name, value:"Sylvie Stone", timestamp: 1340385863990010, ttl: 0},
    {name:devices, value:["MacBook Pro"], timestamp: 1340385863990010, ttl: 0},
    {name:age, value:46, timestamp: 1340133763990010, ttl: 0}
]
</code></pre>

<p>As you&rsquo;ll have imagined, Sylvie is relieved she&rsquo;s not really 46&hellip; the entry is simply older than the rest of them, but was neither deleted nor overwritten!</p>

<p>Every decent driver for Cassandra can expose the timestamps and TTL&rsquo;s as well &ndash; and there&rsquo;s your solution to clean up the mess in the &ldquo;eventually consistent&rdquo; paradigm that the database follows: If it&rsquo;s not the same timestamp as the key, simply discard the column (you&rsquo;re free to delete it as well).</p>

<p>And don&rsquo;t worry, this added hassle in handling something that would be considered a no-brainer with more conventional databases is more than worth the flexibility gained with Cassandra&rsquo;s independent columns. More on advanced data modelling leveraging this power will follow!</p>
]]></content>
  </entry>
  
</feed>
